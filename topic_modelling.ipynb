{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71f59fc5-dcdf-4cad-a892-87ce4eda9cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Load the dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "documents = newsgroups.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bf3063-09e6-4876-9e55-9ef96d44647f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e480afca-66d3-43e2-8ce9-1b9776961303",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f143598c-e318-44e1-b1db-57c0bb6af623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    # Remove non-alphabetical characters and lowercase the text\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text).lower()\n",
    "    # Tokenize and lemmatize\n",
    "    words = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply preprocessing to each document\n",
    "cleaned_documents = [preprocess(doc) for doc in documents]\n",
    "\n",
    "# Convert documents to a document-term matrix\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "doc_term_matrix = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e167c8c3-efae-49a2-8018-f096d2858a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d261d111-56ee-45a8-ac55-5f0a297967df",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Topic Modeling with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b8e9a8d-fd42-4abe-b87a-3d83b3a4c45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.216*\"ax\" + 0.043*\"q\" + 0.038*\"p\" + 0.037*\"f\" + 0.036*\"w\"')\n",
      "(1, '0.019*\"edu\" + 0.015*\"image\" + 0.009*\"mail\" + 0.008*\"com\" + 0.008*\"information\"')\n",
      "(2, '0.010*\"government\" + 0.009*\"state\" + 0.007*\"gun\" + 0.007*\"armenian\" + 0.007*\"people\"')\n",
      "(3, '0.014*\"one\" + 0.010*\"year\" + 0.010*\"get\" + 0.010*\"time\" + 0.009*\"go\"')\n",
      "(4, '0.041*\"x\" + 0.021*\"window\" + 0.020*\"file\" + 0.012*\"do\" + 0.011*\"program\"')\n",
      "(5, '0.014*\"drive\" + 0.011*\"card\" + 0.009*\"one\" + 0.008*\"would\" + 0.008*\"disk\"')\n",
      "(6, '0.037*\"game\" + 0.025*\"team\" + 0.015*\"player\" + 0.011*\"season\" + 0.011*\"play\"')\n",
      "(7, '0.015*\"would\" + 0.012*\"people\" + 0.012*\"one\" + 0.010*\"think\" + 0.008*\"know\"')\n",
      "(8, '0.036*\"god\" + 0.017*\"key\" + 0.011*\"jesus\" + 0.011*\"christian\" + 0.010*\"church\"')\n",
      "(9, '0.007*\"car\" + 0.006*\"use\" + 0.006*\"year\" + 0.005*\"one\" + 0.005*\"new\"')\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Create a dictionary representation of the documents\n",
    "dictionary = corpora.Dictionary([doc.split() for doc in cleaned_documents])\n",
    "\n",
    "# Filter out extremes to limit the number of features\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5)\n",
    "\n",
    "# Convert the corpus into a bag-of-words format\n",
    "corpus = [dictionary.doc2bow(doc.split()) for doc in cleaned_documents]\n",
    "\n",
    "# Apply LDA\n",
    "lda_model = gensim.models.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=15)\n",
    "\n",
    "# Print the topics discovered by LDA\n",
    "topics = lda_model.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af2aa60-4c37-41b3-bdad-08d79d8be749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5ddc001-2226-40ac-838b-d8274ea8fa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualization of Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419e26cc-b6ee-4342-a7ab-462ed3cb2e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create the LDA model\n",
    "lda = LatentDirichletAllocation(n_components=10, random_state=42)\n",
    "lda.fit(doc_term_matrix)\n",
    "\n",
    "# Extract feature names (terms)\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Plot the top words per topic with improved display\n",
    "num_topics = lda.components_.shape[0]\n",
    "words_per_topic = 7  # Show only the top 7 words per topic\n",
    "cols = 2  # Number of columns in the figure\n",
    "rows = (num_topics // cols) + (num_topics % cols)\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, rows * 4), sharex=True, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    if i < num_topics:\n",
    "        top_words = lda.components_[i].argsort()[:-words_per_topic-1:-1]\n",
    "        topic_words = [terms[idx] for idx in top_words]\n",
    "        ax.barh(topic_words, lda.components_[i][top_words], color='skyblue')\n",
    "        ax.set_title(f'Topic {i + 1}', fontsize=16)\n",
    "        ax.tick_params(axis='y', labelsize=12)\n",
    "        ax.tick_params(axis='x', labelsize=10)\n",
    "    else:\n",
    "        ax.axis('off')  # Hide any unused subplots\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffda0391-92f9-4627-b497-2728d988917b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac5addb0-2e78-4f17-8a64-004f1e91cd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Document Similarity with Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae49745-d73f-4514-b73f-bac9b956ae8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=[doc.split() for doc in cleaned_documents], vector_size=100, window=5, min_count=5, workers=4)\n",
    "\n",
    "# Generate document vectors by averaging word vectors\n",
    "def document_vector(doc):\n",
    "    words = [word for word in doc.split() if word in word2vec_model.wv]\n",
    "    if words:  # Check if there are any valid words\n",
    "        return sum(word2vec_model.wv[word] for word in words) / len(words)\n",
    "    else:\n",
    "        return None  # Return None or skip this document\n",
    "\n",
    "# Apply the function to each document\n",
    "doc_vectors = [document_vector(doc) for doc in cleaned_documents if document_vector(doc) is not None]\n",
    "\n",
    "# Calculate cosine similarity between documents\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_matrix = cosine_similarity(doc_vectors)\n",
    "\n",
    "# Print similarity matrix for first 5 documents\n",
    "print(similarity_matrix[:5, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd68b4f-40ef-4fa2-964f-6dd5486c7181",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
